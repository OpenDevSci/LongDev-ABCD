

```{r read-data}
#| echo: TRUE
#| messages: FALSE
#| warning: FALSE
#| output: FALSE
#| cache: FALSE

# Set the data paths
data_path_1 <- "/Users/shawes/ABCD/data/rds/abcd_5.0_rds/demo5.0.rds"
data_path_2 <- "/Users/shawes/ABCD/data/rds/abcd_5.0_rds/core-rds-5.0/non-imaging_excluding_nt_5.0.rds"

# Read the data
data_demographics <- readRDS(data_path_1)
data_nonimaging <- readRDS(data_path_2)

# Subset the nonimaging data to include desired variables
selected_vars <- c("src_subject_id", "eventname", "nihtbx_totalcomp_fc", "anthroweightcalc", "anthroweightcalc")
subset_data <- data_nonimaging[, selected_vars]

library(dplyr)
# # Merge the datasets on 'src_subject_id' and 'eventname'
merged_data <- data_demographics %>%
    full_join(subset_data, by = c("src_subject_id", "eventname"))

# Inspect the merged data structure
str(merged_data)

# Define event names to be retained in the analysis and convert variables to appropriate data types
eventnames_to_include <- c(
    "baseline_year_1_arm_1",
    "1_year_follow_up_y_arm_1",
    "2_year_follow_up_y_arm_1",
    "3_year_follow_up_y_arm_1",
    "4_year_follow_up_y_arm_1"
)

df_temp1 <- merged_data %>%
    filter(eventname %in% eventnames_to_include) %>%
    mutate(
        src_subject_id = as.factor(src_subject_id),
        eventname = factor(eventname, levels = eventnames_to_include, ordered = TRUE),
        age = as.numeric(age),
        sex = as.factor(sex),
        race.4level = as.factor(race.4level),
        hisp = as.factor(hisp),
        high.educ.bl = as.factor(high.educ.bl),
        household.income.bl = as.factor(household.income.bl),
        acs_raked_propensity_score = as.numeric(acs_raked_propensity_score),
        site_id_l = as.factor(site_id_l),
        nihtbx_totalcomp_fc = as.numeric(nihtbx_totalcomp_fc),
        anthroweightcalc = as.numeric(anthroweightcalc),
        anthroweightcalc = as.numeric(anthroweightcalc)
    ) %>%
    # Exclude cases from unused assessment waves
    filter(!is.na(eventname))
```

-----

```{r}
# Creating a randomly drawn subsample
df_sampled_temp <- df_temp1

# Set seed for reproducibility
set.seed(123)

# Calculate the number of rows to sample (30% of the dataset)
sample_size <- floor(0.3 * nrow(df_sampled_temp))

# Randomly sample row indices
sampled_indices <- sample(x = 1:nrow(df), size = sample_size)

# Subset the dataframe using the sampled indices
df_sampled <- df_sampled_temp[sampled_indices, ]

# df_sampled now contains a random 30% cross-section of the full dataset

```

----


```{r}
# Generating random/unique participant IDs
set.seed(123) # For reproducibility

# Create a vector of random unique IDs and directly replace the original ID column
df_sampled$src_subject_id <- sample(10000:99999, size = nrow(df_sampled), replace = FALSE)

df <- df_sampled

# Save df in the current working directory as my_dataframe.rds
saveRDS(df, "FaKEbcd.rds")


```

```{r}
# Install and load the googledrive package
if (!requireNamespace("googledrive", quietly = TRUE)) install.packages("googledrive")
library(googledrive)

# Authenticate with Google Drive


# Specify the file ID and the destination path
file_id <- "1_gnaEx0wJ28tT0CD_B6mPFwngdfSb6Dr" # Replace with your actual file ID
dest_file <- "my_data.rds" # Specify your destination path

# Download the file
drive_download(as_id(file_id), path = dest_file, overwrite = TRUE)

# Load the .rds file for analysis
my_data <- readRDS(dest_file)

# Begin your analysis (example: view the first few rows)
head(my_data)


```